{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900ba4e6",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f76aee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:35:26.011095Z",
     "start_time": "2021-07-31T22:35:25.997093Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install transformers==3.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "083faff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:35:35.370854Z",
     "start_time": "2021-07-31T22:35:26.013095Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries needed\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup, logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, SequentialSampler, RandomSampler, DataLoader\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import gc; gc.enable()\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Setting up the device for GPU usage\n",
    "\n",
    "from torch import cuda\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "793babe9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:41:18.112642Z",
     "start_time": "2021-07-31T22:41:18.103641Z"
    }
   },
   "outputs": [],
   "source": [
    "# Definitions\n",
    "# The model is tuned based on roberta-large\n",
    "MODEL_DIR = 'roberta-large'\n",
    "HIDDEN_SIZE = 1024\n",
    "NUM_HIDDEN_LAYERS = 24\n",
    "\n",
    "MAX_LENGTH = 300\n",
    "LR = 2e-5\n",
    "EPS = 1e-8\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "SEEDS = [66, 17]\n",
    "\n",
    "EPOCHS = 5\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VAL_BATCH_SIZE = 32\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d15f11",
   "metadata": {},
   "source": [
    "## Prepare Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b77b4c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:35:35.433861Z",
     "start_time": "2021-07-31T22:35:35.419655Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eba05ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:35:35.449868Z",
     "start_time": "2021-07-31T22:35:35.434862Z"
    }
   },
   "outputs": [],
   "source": [
    "# K-Fold\n",
    "class ContinuousStratifiedKFold(StratifiedKFold):\n",
    "    def split(selfself, x, y, groups=None):\n",
    "        num_bins = int(np.floor(1 + np.log2(len(y))))\n",
    "        bins = pd.cut(y, bins=num_bins, labels=False)\n",
    "        return super().split(x, bins, groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fda1c8",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "- Dataloader is used to for creating training and validation dataloader that load data to the neural network in a defined manner. This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of dataloaded to the memory and then passed to the neural network needs to be controlled.\n",
    "- This control is achieved using the parameters such as `batch_size` and `max_len`.\n",
    "- Training and Validation dataloaders are used in the training and validation part of the flow respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ce5b269",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:35:35.465871Z",
     "start_time": "2021-07-31T22:35:35.450869Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data_loaders(data, fold):\n",
    "    \n",
    "    x_train = data.loc[data.fold != fold, 'excerpt'].tolist()\n",
    "    y_train = data.loc[data.fold != fold, 'target'].values\n",
    "    x_val = data.loc[data.fold == fold, 'excerpt'].tolist()\n",
    "    y_val = data.loc[data.fold == fold, 'target'].values\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "    \n",
    "    encoded_train = tokenizer.batch_encode_plus(\n",
    "        x_train, \n",
    "        add_special_tokens=True, \n",
    "        return_attention_mask=True, \n",
    "        padding='max_length', \n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    encoded_val = tokenizer.batch_encode_plus(\n",
    "        x_val, \n",
    "        add_special_tokens=True, \n",
    "        return_attention_mask=True, \n",
    "        padding='max_length', \n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    dataset_train = TensorDataset(\n",
    "        encoded_train['input_ids'],\n",
    "        encoded_train['attention_mask'],\n",
    "        torch.tensor(y_train)\n",
    "    )\n",
    "    \n",
    "    dataset_val = TensorDataset(\n",
    "        encoded_val['input_ids'],\n",
    "        encoded_val['attention_mask'],\n",
    "        torch.tensor(y_val)\n",
    "    )\n",
    "    # Create the dataloader\n",
    "    dataloader_train = DataLoader(\n",
    "        dataset_train,\n",
    "        sampler = RandomSampler(dataset_train),\n",
    "        batch_size=TRAIN_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    dataloader_val = DataLoader(\n",
    "        dataset_val,\n",
    "        sampler = SequentialSampler(dataset_val),\n",
    "        batch_size=VAL_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    return dataloader_train, dataloader_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a0c1a",
   "metadata": {},
   "source": [
    "## Create Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dce359c",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    " - We will be creating a neural network with the `MeanPoolingModel`.\n",
    " - Last hidden state `[batch, maxlen, hidden_state]`  is the sequence of hidden-states at the output of the last layer of the model.\n",
    " - In this model, we use the mean embedding in the last hidden state of RoBERTa model to be the word embedding method.\n",
    " - Detailed Explanation in https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa8d67f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:35:35.481880Z",
     "start_time": "2021-07-31T22:35:35.467873Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the Mean Pooling Model\n",
    "class MeanPoolingModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name, config=config)\n",
    "        self.linear = nn.Linear(HIDDEN_SIZE, 1)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        \n",
    "        outputs = self.model(input_ids, attention_mask)\n",
    "        # Get the last hidden state\n",
    "        last_hidden_state = outputs[0]\n",
    "        # Expand Attention Mask from [batch_size, max_len] to [batch_size, max_len, hidden_size]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        # Sum Embeddings along max_len axis so now we have [batch_size, hidden_size]\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        # Sum Mask along max_len axis. This is done so that we can ignore padding tokens.\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        # Take Average\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        # regression head\n",
    "        logits = self.linear(mean_embeddings)\n",
    "        preds = logits.squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n",
    "            return loss\n",
    "        else:\n",
    "            return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927a8b21",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "077d0269",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:35:35.528890Z",
     "start_time": "2021-07-31T22:35:35.483881Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# Create stratified folds\n",
    "kf = ContinuousStratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "for f, (t_, v_) in enumerate(kf.split(data, data.target)):\n",
    "    data.loc[v_, 'fold'] = f\n",
    "    \n",
    "data['fold'] = data['fold'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c7a7d9",
   "metadata": {},
   "source": [
    "### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db32b707",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:35:35.544894Z",
     "start_time": "2021-07-31T22:35:35.530892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the evaluator\n",
    "def evaluate(model, val_dataloader):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    \n",
    "    for batch in val_dataloader:\n",
    "        \n",
    "        batch = tuple(b.to(DEVICE) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            loss = model(**inputs)\n",
    "            \n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "    loss_val_avg = loss_val_total/len(val_dataloader) \n",
    "            \n",
    "    return loss_val_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e611a84",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a883d55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:41:37.035282Z",
     "start_time": "2021-07-31T22:41:37.022278Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the training function\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader):\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr = LR, eps = EPS)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, \n",
    "                                                num_training_steps=len(train_dataloader) * EPOCHS)\n",
    "    best_val_loss = 1\n",
    "    best_model = None\n",
    "    \n",
    "    model.train()                               \n",
    "    for epoch in range(EPOCHS):\n",
    "    \n",
    "        loss_train_total = 0\n",
    "        for batch in tqdm(train_dataloader):\n",
    "    \n",
    "            model.zero_grad()\n",
    "            batch = tuple(b.to(DEVICE) for b in batch)\n",
    "            inputs = {\n",
    "                'input_ids': batch[0],\n",
    "                'attention_mask': batch[1],\n",
    "                'labels': batch[2]\n",
    "            }\n",
    "        \n",
    "            loss = model(**inputs)\n",
    "            loss_train_total += loss.item()\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        loss_train_avg = loss_train_total / len(train_dataloader)\n",
    "        loss_val_avg = evaluate(model, val_dataloader)\n",
    "        print(f'epoch:{epoch+1}/{EPOCHS} train loss={loss_train_avg}  val loss={loss_val_avg}')\n",
    "   \n",
    "        if loss_val_avg < best_val_loss:\n",
    "            best_val_loss = loss_val_avg\n",
    "            best_model = model\n",
    "                       \n",
    "    return best_val_loss, best_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f134f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:41:47.864383Z",
     "start_time": "2021-07-31T22:41:38.065946Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Change to True if you want to tune the model\n",
    "TRAINING=False\n",
    "\n",
    "if TRAINING: \n",
    "    # The model is tune based on the number of seeds\n",
    "    for i, seed in enumerate(SEEDS):\n",
    "\n",
    "        print(f'********* seed({i}) = {seed} ***********')\n",
    "    \n",
    "        for fold in range(NUM_FOLDS):\n",
    "            print(f'*** fold = {fold} ***')\n",
    "            seed_everything(seed)\n",
    "            train_dataloader, val_dataloader = get_data_loaders(data, fold)\n",
    "\n",
    "            model = MeanPoolingModel(MODEL_DIR)\n",
    "            model.to(DEVICE)\n",
    "\n",
    "            loss, best_model = train(model, train_dataloader, val_dataloader)\n",
    "\n",
    "            model_path = f\"model_{seed + 1}_{fold + 1}.pth\"\n",
    "            # Save the tuned model\n",
    "            torch.save(best_model.state_dict(), model_path)\n",
    "\n",
    "            del model, best_model        \n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18739510",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4866a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BATCH_SIZE = 1\n",
    "\n",
    "def get_test_loader(data):\n",
    "\n",
    "    x_test = data.excerpt.tolist()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "\n",
    "    encoded_test = tokenizer.batch_encode_plus(\n",
    "        x_test, \n",
    "        add_special_tokens=True, \n",
    "        return_attention_mask=True, \n",
    "        padding='max_length', \n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    dataset_test = TensorDataset(\n",
    "        encoded_test['input_ids'],\n",
    "        encoded_test['attention_mask']\n",
    "    )\n",
    "\n",
    "    dataloader_test = DataLoader(\n",
    "        dataset_test,\n",
    "        sampler = SequentialSampler(dataset_test),\n",
    "        batch_size=TEST_BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    return dataloader_test\n",
    "\n",
    "test_dataloader = get_test_loader(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfba554",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = [] \n",
    "for seed in SEEDS:\n",
    "    \n",
    "    fold_predictions = []\n",
    "    \n",
    "    for fold in tqdm(range(NUM_FOLDS)):\n",
    "        \n",
    "        model_path = f\"model_{seed + 1}_{fold + 1}.pth\" \n",
    "        print(f\"\\nUsing {model_path}\")   \n",
    "            \n",
    "        model = MeanPoolingModel(MODEL_DIR)\n",
    "        model.load_state_dict(torch.load(model_path)) \n",
    "        model.to(DEVICE)\n",
    "        model.eval()\n",
    "\n",
    "        predictions = []\n",
    "        for batch in test_dataloader:\n",
    "\n",
    "            batch = tuple(b.to(DEVICE) for b in batch)\n",
    "\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels':         None,\n",
    "                     }\n",
    "\n",
    "     \n",
    "            preds = model(**inputs).item()\n",
    "            predictions.append(preds)\n",
    "            \n",
    "        del model \n",
    "        gc.collect()\n",
    "            \n",
    "        fold_predictions.append(predictions)\n",
    "    all_predictions.append(np.mean(fold_predictions, axis=0).tolist())\n",
    "    \n",
    "model_predictions = np.mean(all_predictions,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0048c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(os.path.join(INPUT_DIR, 'sample_submission.csv'))\n",
    "submit.target = model_predictions\n",
    "submit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afce2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
